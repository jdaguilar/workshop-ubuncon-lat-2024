{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Workshop Image](static/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Workshop Ubucon Latinoamerica 2024\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el repositorio para el taller de Ubucon Latinoamerica 2024 titulado:\n",
    "\n",
    "📊 Crea tu propio laboratorio de Big Data con MicroK8s y Spark\n",
    "\n",
    "A continuación se encuentra el enlace al video de la presentación [aquí](https://www.youtube.com/watch?v=kSlLjAXISA8):\n",
    "\n",
    "Recomendamos ver el video de la presentación antes de iniciar el taller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisitos\n",
    "\n",
    "**Disclaimer**: Este proyecto no ejecuta en Windows.\n",
    "\n",
    "**Hardware:**\n",
    "\n",
    "Se recomienda que se ejecute en una máquina externa que minimo tenga:\n",
    "\n",
    "- +8 GB RAM\n",
    "- +2 CPU\n",
    "- +20GB almacenamiento.\n",
    "\n",
    "\n",
    "Se recomienda correr el taller en una máquina con Ubuntu 22.04 o superior. Y adicionamente instalar el paquete de Snap. Para instalar Snap en tu máquina puedes seguir este enlace: [Instalar Snap](https://snapcraft.io/docs/installing-snapd)\n",
    "\n",
    "Para verificar que tienes instalado Snap en tu máquina puedes ejecutar el siguiente comando:\n",
    "```bash\n",
    "snap --version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializar el proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ejecute el archivo `setup.sh` para desplegar los componentes de Kmicrok8s, asi como el cliente de spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "password = getpass.getpass()\n",
    "command = \"sudo -S bash setup.sh\" #can be any command but don't forget -S as it enables input from stdin\n",
    "os.system('echo %s | %s' % (password, command))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Ejecute el archivo `init_project.sh` para poder configurar los buckets de S3 en MinIO y configurar spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash init_project.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cargar códigos de prueba, con el script `upload_artifacts.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash upload_artifacts.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cargar códigos de prueba, con el script `upload_data.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash upload_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para iniciar la terminal de pyspark ejecute en siguiente comando en otra terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-client.pyspark --username spark --namespace spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deberias tener una salida como esta:\n",
    "\n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.4.2\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.10.12 (main, Jul 29 2024 16:56:48)\n",
    "Spark context Web UI available at http://192.168.0.13:4040\n",
    "Spark context available as 'sc' (master = k8s://https://192.168.0.13:16443, app id = spark-6af6f8c7a9104948a91c73443c85333d).\n",
    "SparkSession available as 'spark'.\n",
    ">>>\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imprima un 1 + 1 o un hola mundo (te lo dejo a tu criterio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecute los archivos de Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ejecute el archivo `count_vowels.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-client.spark-submit \\\n",
    "    --username spark --namespace spark \\\n",
    "    --deploy-mode cluster \\\n",
    "    s3a://artifacts/python/count_vowels.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder ver el resultado ejecuta en otra terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_name=$(kubectl get pods -n spark | grep 'count-vowels-.*-driver' | tail -n 1 | cut -d' ' -f1)\n",
    "\n",
    "# View only the line containing the output\n",
    "kubectl logs $pod_name -n spark | grep \"The number of vowels in the string is\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar archivo de procesamiento\n",
    "\n",
    "\n",
    "Ahora bien, vamos a ejecutar un archivo para procesar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-client.spark-submit \\\n",
    "    --username spark --namespace spark \\\n",
    "    --deploy-mode cluster \\\n",
    "    s3a://artifacts/python/process_gh_archive_data.py \\\n",
    "    --source_files_pattern=s3a://raw/gh_archive/year=2024/month=01/day=01/hour=0 \\\n",
    "    --destination_files_pattern=s3a://curated/parquet/gh_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta ejecución crea un archivo curado en formato Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutemos ahora otra hora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-client.spark-submit \\\n",
    "    --username spark --namespace spark \\\n",
    "    --deploy-mode cluster \\\n",
    "    s3a://artifacts/python/process_gh_archive_data.py \\\n",
    "    --source_files_pattern=s3a://raw/gh_archive/year=2024/month=01/day=01/hour=1 \\\n",
    "    --destination_files_pattern=s3a://curated/parquet/gh_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ejecutar un archivo que no guarde en un formato como Parquet, sino en otros sistemas de archivos preparados para Big Data, como Apache Iceberg:\n",
    "\n",
    "Ejecute el siguiente comando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-client.spark-submit \\\n",
    "    --username spark --namespace spark \\\n",
    "    --deploy-mode cluster \\\n",
    "    --packages org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.6.1 \\\n",
    "    s3a://artifacts/python/demo_iceberg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procesa los datos de GH archive, pero en Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-client.spark-submit \\\n",
    "    --username spark --namespace spark \\\n",
    "    --deploy-mode cluster \\\n",
    "    s3a://artifacts/python/process_gh_archive_data_iceberg.py \\\n",
    "    --source_files_pattern=s3a://raw/gh_archive/year=2024/month=01/day=02\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://canonical.com/data/docs/spark/k8s/t-overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
